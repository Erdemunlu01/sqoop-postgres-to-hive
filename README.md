# Sqoop Case Study: PostgreSQL â†’ Apache Hive (Retail DB)

This repository presents an end-to-end **case study** that demonstrates how retail data stored in **PostgreSQL** is transferred into **Apache Hive** using **Apache Sqoop**.

In this study:
- CSV files are loaded into PostgreSQL,
- PostgreSQL tables are imported into Hive using Sqoop (Hive tables are created automatically),
- Hive tables are optimized by converting them into **ORC format with SNAPPY compression**.

> Goal: To document a real-world data pipeline from **source database â†’ data transfer â†’ Hive â†’ performance optimization**.

---

## ğŸš€ Architecture Flow

The overall data flow is as follows:

**CSV Files â†’ PostgreSQL (traindb) â†’ Sqoop Import â†’ Hive (test1.\*) â†’ ORC + SNAPPY (test1.\*_orc)**

This repository contains terminal commands, PostgreSQL DDL scripts, and Hive optimization SQL files that support this flow.

---

## ğŸ“¦ Tables Used

The following tables are transferred as part of this case study:

- `categories`
- `customers`
- `departments`
- `order_items`
- `orders`
- `products`

---

## ğŸ“ Repository Structure

```text
sqoop-postgres-to-hive/
â”œâ”€ README.md
â”‚
â”œâ”€ docs/
â”‚  â”œâ”€ 01_overview.md
â”‚  â”œâ”€ 02_prerequisites.md
â”‚  â”œâ”€ 03_runbook_step_by_step.md
â”‚  â””â”€ troubleshooting.md
â”‚
â”œâ”€ postgres/
â”‚  â””â”€ 01_create_tables.sql
â”‚
â”œâ”€ terminal/
â”‚  â””â”€ 01_sqoop_pipeline_commands.txt
â”‚
â”œâ”€ hive/
â”‚  â””â”€ 02_orc_snappy_optimization.sql
â”‚
â””â”€ diagrams/
   â””â”€ architecture.png
